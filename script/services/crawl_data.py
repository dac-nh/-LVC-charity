# -*- coding: utf-8 -*-
"""CrawlDataWeb_Attribute.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HW4UfSPsHYCYyLxSh7_HvERrAJWO-Duc

# Library
"""

# requests for fetching html of website

import re
from urllib.request import urlopen

import pandas as pd
import requests
from bs4 import BeautifulSoup
from google.colab import drive

drive.mount('/content/gdrive/')

import os

"!pip install underthesea"

"""# General Function"""


# Find true link from a website

def find_true(page_size, url, keyword, before_term, after_term):
    true_link = []
    for i in range(page_size):
        page = requests.get(url[i])
        data = page.text
        soup = BeautifulSoup(data)
        all_link = soup.find_all('a')
        for link in all_link:
            raw_link = link.get('href')
            raw_link = str(raw_link)
            search = re.search(keyword, raw_link)
            if search:
                raw_link = before_term + raw_link + after_term
                if raw_link not in true_link:
                    true_link.append(raw_link)
    return true_link


# ------------------------------------------------------------------------------


# Extract content from a url

def extract_content(url, exclude, tag, attribute):
    true_text = ''
    try:
        html = urlopen(url)
        content = BeautifulSoup(html)
        if len(attribute) == 0:
            all_text = content.find_all(tag)
        else:
            all_text = content.find_all(tag, attrs={"class": attribute})
        for texts in all_text:
            raw_text = str(texts.text)
            for char in exclude:
                raw_text = raw_text.replace(char, " ")
            true_text = true_text + ' ' + raw_text
    except:
        true_text = 'No content found to extract.'
    return true_text


# ------------------------------------------------------------------------------


# Save content from a website

def save_content(page, exclude, tag, attribute):
    size = len(page)
    contents = []
    for i in range(size):
        content = extract_content(page[i], exclude, tag, attribute)
        contents.append(content)
    return contents


# ------------------------------------------------------------------------------


# Create dataframe to save content extracted

def create_df(page_name, page_link, page_content):
    df = pd.DataFrame({'Name': page_name, 'Link': page_link, 'Content': page_content},
                      columns=['Name', 'Link', 'Content'])
    return df


# ------------------------------------------------------------------------------


# Save data csv

def store_data_csv(data, path, name):
    # Save current data file
    list_files = os.listdir(path)
    for file in list_files:
        if file == name + '.csv':
            os.rename(path + file, path + name + '_old' + '.csv')

    # Import crawl data to new csv file
    with open(os.path.join(path, name + '.csv'), 'w') as myfile:
        data.to_csv(myfile, encoding='utf-8')
        status = 'New'

    # Delete ole file if new file created

    if status == 'New':
        list_files = os.listdir(path)
        for file in list_files:
            if file == name + '_old' + '.csv':
                os.remove(path + file)


# ------------------------------------------------------------------------------


# Extract Attribute

def extract_attribute(content, attribute, attribute_data, attribute_word, n_attribute, percent, general_attribute):
    word_frequency = {x: content.count(x) for x in content}
    word_frequency_df = pd.DataFrame(word_frequency.items(), columns=['word', 'frequency'])

    # Exclude stop words

    n = len(word_frequency_df)
    word_array = []
    for i in range(n):
        if word_frequency_df['word'][i] in stopwords:
            word_array.append('stop_word')
        else:
            word_array.append('non_stop_word')

    # Percent of non_stop_word in document

    word_frequency_df['stop_word'] = word_array
    word_frequency_df = word_frequency_df[word_frequency_df['stop_word'] == 'non_stop_word'].drop(['stop_word'],
                                                                                                  axis=1).sort_values(
        'frequency', ascending=False)[:round(len(word_frequency_df) * percent)]
    word_frequency_df = word_frequency_df['word'].tolist()

    # Find attribute

    n_word = len(word_frequency_df)
    to_attribute = []
    for i in range(n_word):
        for j in range(n_attribute):
            if word_frequency_df[i] == attribute_data[attribute_word][j]:
                to_attribute.append(attribute_data[attribute][j])
    if len(to_attribute) == 0:
        to_attribute.append(general_attribute)

    to_attribute = list(dict.fromkeys(to_attribute))

    return (to_attribute)


# ------------------------------------------------------------------------------


# Extract contact link

def find_contact_link(content):
    l = len(content)
    contact_link = []
    for i in range(l):
        x = re.findall('com|vn|org|edu|gmail|yahoo', content[i])
        y = re.findall('[.]', content[i])
        if (x) and (y):
            contact_link.append(content[i])
    if not (contact_link):
        contact_link.append('Vui lòng đọc bài đầy đủ ở link để biết thông tin chi tiết!')
    return (contact_link)


"""# Kindmate Website"""

# kindmate

kindmate_url = []
kindmate_keyword = '/project/'
kindmate_size = 30
for i in range(kindmate_size):
    j = i + 1
    kindmate_url.append("https://kindmate.net/explore?page=" + str(j))

kindmate_link = find_true(kindmate_size, kindmate_url, kindmate_keyword, '', '')
kindmate_exclude = ['\r\n', '\n', '\xa0', '++', '❤️', '&quot']
kindmate_content = save_content(kindmate_link, kindmate_exclude, 'p', [])

"""# Nhidong Website"""

# nhidong.org

nhidong_size = 1
nhidong_url = []
nhidong_url.append("http://nhidong.org.vn/cong-tac-xa-hoi-c1061.aspx")
nhidong_keyword = '/chuyen-muc/'
nhidong_link = find_true(nhidong_size, nhidong_url, nhidong_keyword, "http://nhidong.org.vn", '')
nhidong_tag = 'div'
nhidong_attribute = ['col-md-12 des-ct', 'row content-news']
nhidong_exclude = ['\n', '\xa0']
nhidong_content = save_content(nhidong_link, nhidong_exclude, nhidong_tag, nhidong_attribute)

"""# Traitimchoem Website"""

# Trai tim cho em

heart4u_url = []
heart4u_url.append('http://traitimchoem.vn/ho-so')
heart4u_keyword = 'http://traitimchoem.vn/ho-so/'
heart4u_size = 36
for i in range(heart4u_size):
    j = i + 2
    heart4u_url.append('http://traitimchoem.vn/ho-so/page:' + str(j))

heart4u_size = len(heart4u_url)
heart4u_link = find_true(heart4u_size, heart4u_url, heart4u_keyword, '', '')

heart4u_exclude = ['<div class="col-sm-3 col-xs-6">', '</div>', '\n']
heart4u_tag = 'div'
heart4u_attribute = ['col-sm-9', 'col-sm-3 col-xs-6', 'col-sm-9 col-xs-6']

heart4u_content = save_content(heart4u_link, heart4u_exclude, heart4u_tag, heart4u_attribute)

"""#Data Storing

Save all crawled data to a csv file into google drive.
"""

# Create a input data folder to store crawling data

my_path = "/content/gdrive/My Drive/"
data_path = os.path.join(my_path, 'KindProject_InputData/')

# Combine data

kindmate = create_df('Kindmate', kindmate_link, kindmate_content)
nhidong = create_df('Nhidong', nhidong_link, nhidong_content)
heart4u = create_df('Heart4u', heart4u_link, heart4u_content)
data_crawl = pd.concat([kindmate, nhidong, heart4u], ignore_index=True)

# Store Data

store_data_csv(data_crawl, data_path, 'crawl_content')

"""#Tokenization"""

my_path = "/content/gdrive/My Drive/KindProject_InputData/"
data_path = os.path.join(my_path, 'crawl_content.csv')
stopword_path = os.path.join(my_path, 'general_stopwords.txt')

# Read data from gdrive

df = pd.read_csv(data_path)
df = df.drop(['Unnamed: 0'], axis=1)

df_kindmate = df[df['Name'] == 'Kindmate']
df_heart4u = df[df['Name'] == 'Heart4u']
df_kid = df[df['Name'] == 'Nhidong']

stopwords = open(stopword_path, "r").read().split('\n')
stopwords[0] = stopwords[0].replace('\ufeff', '')

from underthesea import word_tokenize

pattern = re.compile(
    r'[^a-zA-Z_ÀÁÂÃÈÉÊÌÍÒÓÔÕÙÚĂĐĨŨƠàáâãèéêìíòóôõùúăđĩũơƯĂẠẢẤẦẨẪẬẮẰẲẴẶẸẺẼỀỀỂưăạảấầẩẫậắằẳẵặẹẻẽềềếểỄỆỈỊỌỎỐỒỔỖỘỚỜỞỠỢỤỦỨỪễệỉịọỏốồổỗộớờởỡợụủứừỬỮỰỲỴÝỶỸửữựỳỵỷỹ ]',
    re.UNICODE)

"""#Extract Attribute"""

n = len(df)

# Extract Object

object_path = os.path.join(my_path, 'object_words.csv')
object_words = pd.read_csv(object_path)
n_object = len(object_words)

objects = []
for i in range(n):
    content = word_tokenize(pattern.sub('', df['Content'][i].lower()))
    objects.append(extract_attribute(content, 'object', object_words, 'object_word', n_object, 0.2,
                                     'Vui lòng đọc bài đầy đủ ở link để biết thông tin chi tiết!'))

df['Objects'] = objects

# Extract Organization

org_path = os.path.join(my_path, 'org_words.csv')
org_words = pd.read_csv(org_path)
n_org = len(org_words)

n_kindmate = len(df_kindmate)
n_kid = len(df_kid)
n_heart4u = len(df_heart4u)

organizations = []
for i in range(n_kindmate):
    content = word_tokenize(pattern.sub('', df['Content'][i]))
    organizations.append(extract_attribute(content, 'org', org_words, 'org_word', n_org, 1,
                                           'Vui lòng đọc bài đầy đủ ở link để biết thông tin chi tiết!'))

for i in range(n_kid):
    organizations.append('Bệnh viện Nhi đồng 1')

for i in range(n_heart4u):
    organizations.append('Trái tim cho em')

df['Organizations'] = organizations

# Extract contact link

contact_links = []
for i in range(n):
    content = word_tokenize(df['Content'][i])
    contact_links.append(find_contact_link(content))

df['Contact_Link'] = contact_links

store_data_csv(df, my_path, 'content_for_class')
